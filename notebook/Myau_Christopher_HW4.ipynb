{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Last_First_HW4</h1></center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: \n",
    "<br>\n",
    "Github Username: \n",
    "<br>\n",
    "USC ID: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Time Series Classification Part 1: Feature Creation/Extraction (HW3 Rerun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Obtain Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import bootstrapped.bootstrap as bs\n",
    "import bootstrapped.stats_functions as bs_stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the AReM Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../AReM/\"\n",
    "\n",
    "\n",
    "def custom_tt_split(directory, directory_name, count):\n",
    "    ret_list1, ret_list2 = [], []\n",
    "    c = 0\n",
    "    for filename in os.listdir(directory + directory_name):\n",
    "        if \"csv\" in filename:\n",
    "            if c < count:\n",
    "                ret_list1.append(pd.read_csv(directory + directory_name + filename, comment=\"#\", header=None))\n",
    "            else:\n",
    "                ret_list2.append(pd.read_csv(directory + directory_name + filename, comment=\"#\", header=None))\n",
    "            c += 1\n",
    "    return [ret_list1, ret_list2]\n",
    "\n",
    "[bending_test, bending_train] = custom_tt_split(directory, \"bending1/\", 2)\n",
    "[bending_test2, bending_train2] = custom_tt_split(directory, \"bending2/\", 2)\n",
    "[bending_test, bending_train] = [bending_test + bending_test2, bending_train + bending_train2]\n",
    "[cycling_test, cycling_train] = custom_tt_split(directory, \"cycling/\", 3)\n",
    "[lying_test, lying_train] = custom_tt_split(directory, \"lying/\", 3)\n",
    "[sitting_test, sitting_train] = custom_tt_split(directory, \"sitting/\", 3)\n",
    "[standing_test, standing_train] = custom_tt_split(directory, \"standing/\", 3)\n",
    "[walking_test, walking_train] = custom_tt_split(directory, \"walking/\", 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [bending_test, cycling_test, lying_test, sitting_test, standing_test, walking_test]\n",
    "train_data = [bending_train, cycling_train, lying_train, sitting_train, standing_train, walking_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_extraction(t_data, columns, sortedIndex):\n",
    "    metrics_df = pd.DataFrame({}, columns = columns)\n",
    "    bending = \"bending\"\n",
    "    for data in t_data:\n",
    "        for df in data:\n",
    "            instance_metrics = []\n",
    "            for i in range(1, 7): \n",
    "                metrics = list(df[i].describe())\n",
    "                metrics = [metrics[m] for m in range(0, len(metrics)) if m not in [0, 5]]\n",
    "                metrics.insert(3, df[i].median())\n",
    "                \n",
    "                # goal: min, max, mean, median, std, 25, 75\n",
    "                #      mean, std, min, median, 25, 75, max\n",
    "                metrics = [metrics[i] for i in sortedIndex]\n",
    "                \n",
    "                instance_metrics = instance_metrics + metrics\n",
    "            instance_metrics.append(bending)\n",
    "            metrics_df.loc[len(metrics_df)] = instance_metrics\n",
    "        if bending == \"bending\":\n",
    "            bending = \"non-bending\"\n",
    "    return metrics_df\n",
    "\n",
    "\n",
    "metric_names, metric_offset = [\"avg_rss12\",\"var_rss12\",\"avg_rss13\",\"var_rss13\",\"avg_rss23\",\"var_rss23\"], 1\n",
    "column_names = [\"min\", \"max\", \"mean\", \"median\", \"std\", \"first_quartile\", \"third_quartile\"]\n",
    "sortedIndex = [2, 6, 0, 3, 1, 4, 5]\n",
    "column_names_expanded = [f\"{c}{i}\" for i in range(metric_offset, len(metric_names) + metric_offset) for c in column_names ]\n",
    "column_names_expanded.append(\"class\")\n",
    "\n",
    "train_metrics_df = metrics_extraction(train_data, column_names_expanded, sortedIndex)\n",
    "test_metrics_df = metrics_extraction(test_data, column_names_expanded, sortedIndex)\n",
    "    \n",
    "train_confidence_intervals = {}\n",
    "test_confidence_intervals = {}\n",
    "\n",
    "\n",
    "for feature in train_metrics_df:\n",
    "    if feature != \"class\":\n",
    "        train_confidence_intervals[feature] = bs.bootstrap(train_metrics_df[feature].to_numpy(), stat_func=bs_stats.std, alpha=0.1)\n",
    "    \n",
    "for feature in test_metrics_df:\n",
    "    if feature != \"class\":\n",
    "        test_confidence_intervals[feature] = bs.bootstrap(test_metrics_df[feature].to_numpy(), stat_func=bs_stats.std, alpha=0.1)\n",
    "    \n",
    "train_extracted_df = train_metrics_df[[\"std1\", \n",
    "                        \"first_quartile1\", \n",
    "                        \"third_quartile1\",\n",
    "                        \"std2\", \n",
    "                        \"first_quartile2\", \n",
    "                        \"third_quartile2\",\n",
    "                        \"std3\", \n",
    "                        \"first_quartile3\", \n",
    "                        \"third_quartile3\",\n",
    "                        \"std4\", \n",
    "                        \"first_quartile4\", \n",
    "                        \"third_quartile4\",\n",
    "                        \"std5\", \n",
    "                        \"first_quartile5\", \n",
    "                        \"third_quartile5\",\n",
    "                        \"std6\", \n",
    "                        \"first_quartile6\", \n",
    "                        \"third_quartile6\",\n",
    "                        \"class\"]]\n",
    "\n",
    "test_extracted_df = train_metrics_df[[\"std1\", \n",
    "                        \"first_quartile1\", \n",
    "                        \"third_quartile1\",\n",
    "                        \"std2\", \n",
    "                        \"first_quartile2\", \n",
    "                        \"third_quartile2\",\n",
    "                        \"std3\", \n",
    "                        \"first_quartile3\", \n",
    "                        \"third_quartile3\",\n",
    "                        \"std4\", \n",
    "                        \"first_quartile4\", \n",
    "                        \"third_quartile4\",\n",
    "                        \"std5\", \n",
    "                        \"first_quartile5\", \n",
    "                        \"third_quartile5\",\n",
    "                        \"std6\", \n",
    "                        \"first_quartile6\", \n",
    "                        \"third_quartile6\",\n",
    "                        \"class\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time Series Classification Part 2: Binary and Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Binary Classification Using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### i. Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take selected features from time series 1, 2, 6\n",
    "\n",
    "train_filtered_df = train_extracted_df[[\"std1\", \n",
    "                        \"first_quartile1\", \n",
    "                        \"third_quartile1\",\n",
    "                        \"std2\", \n",
    "                        \"first_quartile2\", \n",
    "                        \"third_quartile2\",\n",
    "                        \"std6\", \n",
    "                        \"first_quartile6\", \n",
    "                        \"third_quartile6\",\n",
    "                        \"class\"]]\n",
    "\n",
    "sns.pairplot(train_filtered_df, hue=\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ii. Splitted Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = {\"bending\":bending_test, \"cycling\":cycling_test, \"lying\":lying_test, \"sitting\":sitting_test, \"standing\":standing_test, \"walking\":walking_test}\n",
    "train_data = {\"bending\":bending_train, \"cycling\":cycling_train, \"lying\":lying_train, \"sitting\":sitting_train, \"standing\":standing_train, \"walking\":walking_train}\n",
    "\n",
    "train_data_r=pd.DataFrame({})\n",
    "test_data_r=pd.DataFrame({})\n",
    "\n",
    "def reformat(dct):\n",
    "    class_list, exertype_list, instance_list = [], [], []\n",
    "    instance = 0\n",
    "    output_df = pd.DataFrame({})\n",
    "    for exertype in dct.keys():\n",
    "        for six_series in dct[exertype]:\n",
    "            series = six_series.T.apply(lambda row: row.tolist(), axis=1)\n",
    "            meta_series = pd.Series([instance, exertype])\n",
    "            series = series.append(meta_series).drop(0).reset_index(drop=True)\n",
    "            output_df = output_df.append(series, ignore_index=True)\n",
    "            instance += 1\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def split_list(lst):\n",
    "    n = len(lst) // 2\n",
    "    return lst[:n], lst[n:]\n",
    "\n",
    "def split_dataframe(df):\n",
    "    output_df = pd.DataFrame({})\n",
    "    for index, row in df.iterrows():\n",
    "        row_t = row.iloc[:-1]\n",
    "        row_t = row_t.apply(split_list).apply(pd.Series)\n",
    "        row_t = pd.concat([row_t[0], row_t[1], pd.Series(row.iloc[-1])]).reset_index(drop=True)\n",
    "        output_df = output_df.append(row_t, ignore_index=True)\n",
    "    return output_df\n",
    "\n",
    "def metric_calc(input_df):\n",
    "    df = pd.DataFrame({})\n",
    "    metric_df = input_df.iloc[:, :-1]\n",
    "    means = metric_df.apply(lambda x: x.apply(lambda y: np.mean(y)))\n",
    "    medians = metric_df.apply(lambda x: x.apply(lambda y: np.median(y)))\n",
    "    maxes = metric_df.apply(lambda x: x.apply(lambda y: np.max(y)))\n",
    "    mins = metric_df.apply(lambda x: x.apply(lambda y: np.min(y)))\n",
    "    stds = metric_df.apply(lambda x: x.apply(lambda y: np.std(y)))\n",
    "    maxes = metric_df.apply(lambda x: x.apply(lambda y: np.median(y)))\n",
    "    one_quarts = metric_df.apply(lambda x: x.apply(lambda y: np.percentile(y, 25)))\n",
    "    three_quarts = metric_df.apply(lambda x: x.apply(lambda y: np.percentile(y, 75)))\n",
    "    \n",
    "    metrics = {\n",
    "         \"min\":mins, \n",
    "         \"max\":maxes, \n",
    "         \"mean\":means, \n",
    "         \"median\":medians, \n",
    "         \"std\":stds, \n",
    "         \"first quartile\":one_quarts, \n",
    "         \"third quartile\":three_quarts\n",
    "     }\n",
    "    \n",
    "    columns = []\n",
    "    for i in range(len(input_df.columns) - 1):\n",
    "        for m in metrics:\n",
    "            spread_df = metrics[m][i]\n",
    "            df = pd.concat([df, spread_df], axis = 1)\n",
    "        columns += [str(k) + str(i + 1) for k in metrics.keys()]\n",
    "    df.columns = columns\n",
    "    df[\"exercise\"] = input_df.iloc[:,-1:]\n",
    "    return df\n",
    "\n",
    "train_data_r = reformat(train_data)\n",
    "test_data_r = reformat(test_data)\n",
    "\n",
    "train_data_split = split_dataframe(train_data_r)\n",
    "test_data_split = split_dataframe(test_data_r)\n",
    "\n",
    "train_metric_r = metric_calc(train_data_split)\n",
    "test_metric_r = metric_calc(test_data_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filtered_df = train_metric_r[[\n",
    "                        \"std1\", \n",
    "                        \"first quartile1\", \n",
    "                        \"third quartile1\",\n",
    "                        \"std2\", \n",
    "                        \"first quartile2\", \n",
    "                        \"third quartile2\",\n",
    "                        \"std6\", \n",
    "                        \"first quartile6\", \n",
    "                        \"third quartile6\",\n",
    "                        \"std7\", \n",
    "                        \"first quartile7\", \n",
    "                        \"third quartile7\",\n",
    "                        \"std8\", \n",
    "                        \"first quartile8\", \n",
    "                        \"third quartile8\",\n",
    "                        \"std12\", \n",
    "                        \"first quartile12\", \n",
    "                        \"third quartile12\",       \n",
    "                        \"exercise\"]]\n",
    "\n",
    "train_filtered_df.loc[train_filtered_df['exercise'] != 'bending', 'exercise'] = 'non-bending'\n",
    "\n",
    "\n",
    "sns.pairplot(train_filtered_df, hue=\"exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of instances 1, 2, 6 closely mirror the values of 7, 8, and 12. This is most apparent when looking down the diagonal of the graph, we can see how the distribution of bending and non-bending closely match the half of the feature it was sliced from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iii. Time Series Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list_s(lst, s):\n",
    "    split_size = len(lst) // s    \n",
    "    result = []\n",
    "    start = 0\n",
    "    for i in range(s):\n",
    "        end = start + split_size\n",
    "        result.append(lst[start:end])\n",
    "        start = end\n",
    "    return result\n",
    "\n",
    "def split_dataframe_n(df, n):\n",
    "    output_df = pd.DataFrame({})\n",
    "    for index, row in df.iterrows():\n",
    "        row_t = row.iloc[:-1]\n",
    "        row_t = row_t.apply(split_list_s, s=n).apply(pd.Series)\n",
    "        row_t = row_t.stack()\n",
    "        row_t = row_t.append(pd.Series(row.iloc[:-1]))\n",
    "        #-1 not the exercise?\n",
    "        output_df = output_df.append(row_t, ignore_index=True)\n",
    "    output_df.columns = [x for x in range(len(output_df.columns))]\n",
    "    return output_df\n",
    "\n",
    "def metric_calc(input_df):\n",
    "    df = pd.DataFrame({})\n",
    "    metric_df = input_df.iloc[:, :-1]\n",
    "    means = metric_df.apply(lambda x: x.apply(lambda y: np.mean(y)))\n",
    "    medians = metric_df.apply(lambda x: x.apply(lambda y: np.median(y)))\n",
    "    maxes = metric_df.apply(lambda x: x.apply(lambda y: np.max(y)))\n",
    "    mins = metric_df.apply(lambda x: x.apply(lambda y: np.min(y)))\n",
    "    stds = metric_df.apply(lambda x: x.apply(lambda y: np.std(y)))\n",
    "    maxes = metric_df.apply(lambda x: x.apply(lambda y: np.median(y)))\n",
    "    one_quarts = metric_df.apply(lambda x: x.apply(lambda y: np.percentile(y, 25)))\n",
    "    three_quarts = metric_df.apply(lambda x: x.apply(lambda y: np.percentile(y, 75)))\n",
    "    \n",
    "    metrics = {\n",
    "         \"min\":mins, \n",
    "         \"max\":maxes, \n",
    "         \"mean\":means, \n",
    "         \"median\":medians, \n",
    "         \"std\":stds, \n",
    "         \"first quartile\":one_quarts, \n",
    "         \"third quartile\":three_quarts\n",
    "     }\n",
    "    \n",
    "    columns = []\n",
    "    for i in range(len(input_df.columns) - 1):\n",
    "        for m in metrics:\n",
    "            spread_df = metrics[m][i]\n",
    "            df = pd.concat([df, spread_df], axis = 1)\n",
    "        columns += [str(k) + str(i + 1) for k in metrics.keys()]\n",
    "    df.columns = columns\n",
    "    return df\n",
    "\n",
    "train_onetotwenty = [train_data_r]\n",
    "test_onetotwenty = [test_data_r]\n",
    "\n",
    "train_exercise_series = train_data_r[6]\n",
    "test_exercise_series = test_data_r[6]\n",
    "\n",
    "for i in range(19):\n",
    "    train_onetotwenty.append(pd.concat([split_dataframe_n(train_data_r, i+1), train_exercise_series], axis = 1))\n",
    "    test_onetotwenty.append(pd.concat([split_dataframe_n(test_data_r, i+1), test_exercise_series], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics_20 = []\n",
    "test_metrics_20 = []\n",
    "for i in range(20):\n",
    "    train_metrics_20.append(pd.concat([metric_calc(train_onetotwenty[i]), train_exercise_series], axis = 1).rename(columns={6:\"exercise\"}))\n",
    "    test_metrics_20.append(pd.concat([metric_calc(test_onetotwenty[i]), test_exercise_series], axis = 1).rename(columns={6:\"exercise\"}))\n",
    "    train_metrics_20[i].loc[train_metrics_20[i]['exercise'] != 'bending', 'exercise'] = 'non-bending'\n",
    "    test_metrics_20[i].loc[test_metrics_20[i]['exercise'] != 'bending', 'exercise'] = 'non-bending'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "selected_features_list = []\n",
    "\n",
    "for i in range(20):\n",
    "    \n",
    "    # define the set of keywords to select\n",
    "    keywords = {'first', 'third', \"std\"}\n",
    "\n",
    "    # create a regular expression pattern to match any keyword\n",
    "    pattern = '|'.join(keywords)\n",
    "    X = train_metrics_20[i].filter(regex=pattern)\n",
    "    y = train_metrics_20[i]['exercise']\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    lr = LogisticRegression(random_state=1, max_iter=500).fit(X_scaled, y)\n",
    "    sfs = SequentialFeatureSelector(lr,\n",
    "                                n_features_to_select=1, # remove one feature at a time\n",
    "                                direction='backward', # perform backward selection\n",
    "                                cv=5, # number of cross-validation folds\n",
    "                                scoring='roc_auc')\n",
    "    sfs.fit(X, y)\n",
    "    selected_features_list.append(sfs.get_support(indices=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iv. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### v. Test Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### vi. Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### vii. Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Binary Classification Using L1-penalized logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### i. Time Series Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ii. Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Multi-class Classification (The Realistic Case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### i. Time Series Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ii. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ii. Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ISLR 4.8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ISLR 4.8.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Extra Practice ISLR 4.8.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Extra Practice ISLR 4.8.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.435px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "3c20c2d94d2527936fe0f3a300eb11db30fed84423423838e2f93b74eb7aaebc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
